{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3Pvjw3ZlPnz",
        "outputId": "95f30ae6-35ef-4750-b8df-b54fb9f91cc8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m71.7/73.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (75.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.26.4)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp310-cp310-linux_x86_64.whl size=4296187 sha256=96a1fd6d13b186f7d2b56178af8d6222d1943b8c658b249b02b4ed3f8de1b5b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/a2/00/81db54d3e6a8199b829d58e02cec2ddb20ce3e59fad8d3c92a\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-2.13.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext.util\n",
        "# Жарым саат ~ бир саат жүктөйт\n",
        "fasttext.util.download_model('ky', if_exists='ignore')  # Kyrgyz\n",
        "model = fasttext.load_model('cc.ky.300.bin')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53hj_OUwmaea",
        "outputId": "02451597-2053-4526-8e97-2c50bd37a83f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ky.300.bin.gz\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "horse = model.get_sentence_vector('Ат күчтүү жана ылдам.')"
      ],
      "metadata": {
        "id": "qKvAJUvwlPKX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name = model.get_sentence_vector(\"Биздин топко жаңы адамдын атын кош.\")"
      ],
      "metadata": {
        "id": "bopvKVeVtvDQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity = cosine_similarity([horse], [name])\n",
        "print(similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyZqLHpzuONk",
        "outputId": "c5db3b47-97d3-4ea2-c916-ca028ef188b9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.37710795]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "horse2 = model.get_sentence_vector(\"Ат тосмодон секирип өттү.\")\n",
        "\n",
        "similarity = cosine_similarity([horse], [horse2])\n",
        "print(similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93v0q67Iuf7c",
        "outputId": "e7e6f3d8-2863-444a-e46a-2173405b17ff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.47417092]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name2 = model.get_sentence_vector(\"Клубдун атын ким ойлоп тапты?\")\n",
        "\n",
        "similarity = cosine_similarity([name], [name2])\n",
        "print(similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF3ZNg-Nu9J3",
        "outputId": "e38a3cec-cc78-4ae4-8194-dd969c2c3ef8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.5634349]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "similarity = cosine_similarity([horse2], [name2])\n",
        "print(similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qjp0fht4vRtC",
        "outputId": "9a96bc0d-1342-4d33-eca2-90ecc7534049"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.2855695]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "horse_sentences = [\n",
        "    \"Ат талаада чуркап баратат.\", \"Кечээ мен ат миндим.\", \"Атты сатып алгам.\",\n",
        "    \"Дыйкан атка жем берүүдө.\", \"Ат тосмодон секирип өттү.\", \"Аттар сулуу жаныбарлар.\",\n",
        "    \"Аттын жалынын кылдары бекем болот.\", \"Ат чөп жейт, бирок жемди көбүрөөк жакшы көрөт.\",\n",
        "    \"Балдар ат минип жүргөндү жакшы көрүшөт.\", \"Ат катуу кишенеди.\", \"Ал жаңы ат сатып алды.\",\n",
        "    \"Ат мөңкүп жатып, чабарманды ыргытып жиберди.\", \"Биз жапайы ат көрдүк.\", \"Кара ат жарышта жеңди.\",\n",
        "    \"Ат сарайда оонап жатат!\", \"Ал аттын жүнүн тарап койду.\", \"Ат арабаны тартып баратат.\", \"Алардын үч аты бар.\",\n",
        "    \"Ат күчтүү жана ылдам.\", \"Ат жана чабандес бирдей кыймылдады.\"\n",
        "]"
      ],
      "metadata": {
        "id": "tL-uxVjrvrTD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences = [\"Чаптырып жүргөн атын уурдатып алыптыр\", \"Аты ала качып кетип, жыгылган экен\", \"Ашка ат союлду.\"]"
      ],
      "metadata": {
        "id": "YP0yFEqLxgAL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = [model.get_sentence_vector(sentence) for sentence in horse_sentences]"
      ],
      "metadata": {
        "id": "m4NL4q7zy1qT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "pairs = list(itertools.combinations(embeddings, 2))\n",
        "similarities = [cosine_similarity([pair[0]], [pair[1]])[0][0] for pair in pairs]\n",
        "# Calculate average similarity\n",
        "average_similarity = sum(similarities) / len(similarities)\n",
        "print(f\"Average Similarity Score: {average_similarity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bqKr-tMz81h",
        "outputId": "0d3e34d4-0d9c-4823-f6db-ba2cce6a7a1f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Similarity Score: 0.38370108668643393\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shoot_sentences = [\"Сарбаз бутага так атты.\", \"Жаачы жебе атканга даярдады.\", \"Мергенчи бугуну атпай койду.\",\n",
        "\"Ал пистолетти колго алып, ат деп буйрук берди.\", \"Жоокерлер душманды көргөндө ата башташты.\",\n",
        "\"Күзөтчү абага атты.\", \"Мергенчи мылтык менен кушту атты.\", \"Ал бутага көздөй атты.\", \"Бала ойунчук куралдан суу атты.\",\n",
        "\"Башаламандыктарда бир нече жолу ок атылган.\", \"Жакын келбегиле, атам!\", \"Душмандарды коркутуу үчүн асманга ок атылды.\",\n",
        "\"Тынч эмес аймакта бир нече жолу атышуу болду.\", \"Күч органдары кылмышкерди атып салышкан.\", \"Ал ызы-чуу чыкканда абага ок атты.\",\n",
        "\"Полиция шектүүнү аткан жок.\", \"Совет бийлигинин буйругу менен атууга кеткет.\", \"Аскерлер буйрук күтүп, атпай турду.\", \"Кечке бутага аттык.\",\n",
        "\"Лагерге айдалып, андан ары атылып кеткен.\"]"
      ],
      "metadata": {
        "id": "Yyoz7HJ30LmF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = [model.get_sentence_vector(sentence) for sentence in shoot_sentences]"
      ],
      "metadata": {
        "id": "5a3xWelg1Bqj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "pairs = list(itertools.combinations(embeddings, 2))\n",
        "similarities = [cosine_similarity([pair[0]], [pair[1]])[0][0] for pair in pairs]\n",
        "# Calculate average similarity\n",
        "average_similarity = sum(similarities) / len(similarities)\n",
        "print(f\"Average Similarity Score: {average_similarity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aN7VLrK1FCx",
        "outputId": "1add8788-a7ef-4244-af94-06caa61e2268"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Similarity Score: 0.4510482430458069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shoot_sent = model.get_sentence_vector(shoot_sentences[5])\n",
        "horse_sent = model.get_sentence_vector(horse_sentences[7])\n",
        "sim = cosine_similarity([shoot_sent], [horse_sent])\n",
        "print(sim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoSLSRvs1GiD",
        "outputId": "8f7aa2b3-438e-4505-f971-349c82520866"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.46285263]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shoot_sentences[14]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aOo7iP8T2qVl",
        "outputId": "13ad41e1-9913-4183-b50f-ccf83b77263a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ал ызы-чуу чыкканда абага ок атты.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "horse_sentences[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VDax9i4T6XC3",
        "outputId": "c5ff42fc-21b9-4130-d0f1-c052a76237d8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Атты сатып алгам.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shoot_sent = model.get_sentence_vector(shoot_sentences[14])\n",
        "horse_sent = model.get_sentence_vector(horse_sentences[2])\n",
        "sim = cosine_similarity([shoot_sent], [horse_sent])\n",
        "print(sim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uN5iXVm-1O-X",
        "outputId": "8c3c3c8a-efbf-47d8-b416-cf73fe067535"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.31485605]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We want to use a signal phrase that should check if it's closer to \"horses\" or \"shooting\""
      ],
      "metadata": {
        "id": "hh5j1ncr7G7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "signal_phrase_horse = \"жем берди\""
      ],
      "metadata": {
        "id": "TT416I-L6tXW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shoot_sent = model.get_sentence_vector(shoot_sentences[14])\n",
        "signal_phrase_horse_vec = model.get_sentence_vector(signal_phrase_horse)\n",
        "sim = cosine_similarity([shoot_sent], [signal_phrase_horse_vec])\n",
        "print(sim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go3gJMVC61qN",
        "outputId": "cad897fe-0413-448d-a379-a76b817136ba"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.25874266]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "horse_sent = model.get_sentence_vector(horse_sentences[2])\n",
        "sim = cosine_similarity([horse_sent], [signal_phrase_horse_vec])\n",
        "print(sim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1qdrHFV44sr",
        "outputId": "f5acdfbd-f498-489a-9644-6b44dd100d2b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.41549802]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use multiple signal phrases\n",
        "\n",
        "1. We have a dictionary of homonyms. Against every meaning there are 1) example sentences 2) multiple signal phrases: [\"чаптыруу\", \"ээр токун\", \"улак тартыш\"]\n",
        "2. Every time we get a sentence containing an ambigious word, we get its embeddings and compare against signal phrases of every meaning and see which one wins."
      ],
      "metadata": {
        "id": "cdu5w1mQ8Swo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ambigious_words = {\n",
        "    \"ат\": {\n",
        "        \"name\": {\n",
        "            \"signal_phrases\": [\"ысым бер\", \"фамилия\", \"адам\"],\n",
        "        },\n",
        "        \"horse\": {\n",
        "            \"signal_phrases\": [\"чаптыруу\", \"ээр токун\", \"улак тартыш\"],\n",
        "        },\n",
        "        \"shoot\": {\n",
        "            \"signal_phrases\": [\"ок атуу\", \"мылтык\", \"автомат\", \"пистолет\", \"жаа\", \"аскер\", \"полиция\"],\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "5vt_Kb0t7gb9"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_avg_sim(sent_embedding, data):\n",
        "    signal_phrases = data.get('signal_phrases', [])\n",
        "    if not signal_phrases:\n",
        "        return 0  # Return zero if there are no signal phrases\n",
        "    sims = []\n",
        "    for phrase in signal_phrases:\n",
        "        phrase_embedding = model.get_sentence_vector(phrase)\n",
        "        sim = cosine_similarity([sent_embedding], [phrase_embedding])[0][0]\n",
        "        sims.append(sim)\n",
        "    avg_sim = sum(sims) / len(sims)\n",
        "    return avg_sim\n",
        "\n",
        "def disambiguator(word, sentence):\n",
        "    amb_word = ambigious_words.get(word)\n",
        "    if amb_word is None:\n",
        "        raise Exception(f\"Word '{word}' not found in our dictionary\")\n",
        "    results = []\n",
        "    sent_embedding = model.get_sentence_vector(sentence)\n",
        "    for meaning, data in amb_word.items():\n",
        "        avg_score = calculate_avg_sim(sent_embedding, data)\n",
        "        results.append((meaning, avg_score))\n",
        "    # Sort the results in descending order of average similarity\n",
        "    results = sorted(results, key=lambda x: x[1], reverse=True)\n",
        "    # Return the meaning with the highest average similarity score\n",
        "    return results[0][0]"
      ],
      "metadata": {
        "id": "ahDNMOBQFDsu"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlTZE7FBFFCY",
        "outputId": "faed503e-7797-440c-cf9c-579ad42c6465"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__class__',\n",
              " '__contains__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__getitem__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_labels',\n",
              " '_words',\n",
              " 'f',\n",
              " 'get_analogies',\n",
              " 'get_dimension',\n",
              " 'get_input_matrix',\n",
              " 'get_input_vector',\n",
              " 'get_label_id',\n",
              " 'get_labels',\n",
              " 'get_line',\n",
              " 'get_meter',\n",
              " 'get_nearest_neighbors',\n",
              " 'get_output_matrix',\n",
              " 'get_sentence_vector',\n",
              " 'get_subword_id',\n",
              " 'get_subwords',\n",
              " 'get_word_id',\n",
              " 'get_word_vector',\n",
              " 'get_words',\n",
              " 'is_quantized',\n",
              " 'labels',\n",
              " 'predict',\n",
              " 'quantize',\n",
              " 'save_model',\n",
              " 'set_args',\n",
              " 'set_matrices',\n",
              " 'test',\n",
              " 'test_label',\n",
              " 'words']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_nearest_neighbors(\"атты\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLb_4frDFcAO",
        "outputId": "dc3b4049-6e1b-48fd-9baf-902403664f37"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.6032174825668335, 'аттыбы'),\n",
              " (0.5557559132575989, 'аттыш'),\n",
              " (0.5520542860031128, 'аттым'),\n",
              " (0.5266836285591125, 'аттыга'),\n",
              " (0.5206305980682373, 'аттыңыз'),\n",
              " (0.5135608315467834, 'чапчуу'),\n",
              " (0.501383364200592, 'аттың'),\n",
              " (0.47900187969207764, 'аттысы'),\n",
              " (0.4695969521999359, 'аттык'),\n",
              " (0.4694654941558838, 'аттырбай')]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "b_exeoxpFg8N"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_file = \"sentences.json\"\n",
        "with open(json_file, 'r', encoding='utf-8') as f:\n",
        "    sentences = json.load(f)\n",
        "\n",
        "\n",
        "json_file_ambiguous_words_50 = \"ambigious_words.json\"\n",
        "with open(json_file_ambiguous_words_50, 'r', encoding='utf-8') as f:\n",
        "    ambiguous_words_50 = json.load(f)\n",
        "\n",
        "\n",
        "set(sentences.keys()) == set(ambiguous_words_50.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9N_DJ5RDZRxY",
        "outputId": "ed20165d-34ed-44b9-9bc7-9d95c8db2969"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge JSONs\n",
        "ambigious_words = ambiguous_words_50.copy()\n",
        "for amb_word, meanings in ambiguous_words_50.items():\n",
        "    if set(meanings.keys()) != set(sentences[amb_word].keys()):\n",
        "        print(f\"containers of amb_word '{amb_word}' in json mismatch! This key is deleted.\")\n",
        "        del ambigious_words[amb_word]\n",
        "        continue\n",
        "    for meaning in meanings.keys():\n",
        "        ambigious_words[amb_word][meaning][\"sentences\"] = sentences[amb_word][meaning][\"sentences\"]\n",
        "\n",
        "# Save to json\n",
        "with open('ambigious_words_merged.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(ambigious_words, f, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "id": "evCAqB1zZgFO"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach #1: compare sentence embeddings with each phrase and take average similarity score"
      ],
      "metadata": {
        "id": "LgGeKnSYfnrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "count_correct = 0\n",
        "count_all = 0\n",
        "n = len(ambigious_words.keys())\n",
        "for amb_word, meanings in ambigious_words.items():\n",
        "    print(f\"{count_all} / {n}: amb_word = {amb_word}\")\n",
        "    for meaning, val in meanings.items():\n",
        "        for sent in val[\"sentences\"]:\n",
        "            answer = disambiguator(amb_word, sent)\n",
        "            count_all += 1\n",
        "            if answer == meaning:\n",
        "                count_correct += 1\n",
        "#     print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-4JJMC0Zllk",
        "outputId": "ea238463-bc48-4781-d69a-dc564071d168"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 / 43: amb_word = ач\n",
            "2 / 43: amb_word = же\n",
            "4 / 43: amb_word = ак\n",
            "6 / 43: amb_word = кап\n",
            "9 / 43: amb_word = ала\n",
            "12 / 43: amb_word = кеч\n",
            "14 / 43: amb_word = кош\n",
            "16 / 43: amb_word = кал\n",
            "18 / 43: amb_word = бай\n",
            "20 / 43: amb_word = сай\n",
            "22 / 43: amb_word = арык\n",
            "24 / 43: amb_word = кой\n",
            "26 / 43: amb_word = ай\n",
            "29 / 43: amb_word = топ\n",
            "31 / 43: amb_word = жар\n",
            "34 / 43: amb_word = тил\n",
            "47 / 43: amb_word = каз\n",
            "49 / 43: amb_word = там\n",
            "51 / 43: amb_word = жаш\n",
            "54 / 43: amb_word = кара\n",
            "56 / 43: amb_word = мал\n",
            "58 / 43: amb_word = сөз\n",
            "60 / 43: amb_word = бас\n",
            "62 / 43: amb_word = тек\n",
            "64 / 43: amb_word = уч\n",
            "66 / 43: amb_word = жең\n",
            "68 / 43: amb_word = курак\n",
            "70 / 43: amb_word = айт\n",
            "72 / 43: amb_word = түш\n",
            "75 / 43: amb_word = кур\n",
            "78 / 43: amb_word = тай\n",
            "82 / 43: amb_word = кол\n",
            "84 / 43: amb_word = күн\n",
            "86 / 43: amb_word = ат\n",
            "89 / 43: amb_word = жаз\n",
            "93 / 43: amb_word = кат\n",
            "106 / 43: amb_word = сан\n",
            "108 / 43: amb_word = чал\n",
            "112 / 43: amb_word = кир\n",
            "115 / 43: amb_word = чек\n",
            "117 / 43: amb_word = бак\n",
            "119 / 43: amb_word = аябай\n",
            "121 / 43: amb_word = ачуу\n",
            "CPU times: user 2.92 s, sys: 43 ms, total: 2.96 s\n",
            "Wall time: 3.03 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Accuracy = {count_correct / count_all}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaEfzkZCZnj1",
        "outputId": "f5e79019-d6f7-4d8c-c09a-ec3f2aad84ac"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 0.5121951219512195\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_sentence_embedding(sentence):\n",
        "    words = sentence.split()  # Tokenize the sentence into words\n",
        "    word_vectors = [model.get_word_vector(word) for word in words if word in model]\n",
        "\n",
        "    if len(word_vectors) == 0:\n",
        "        return np.zeros(300)  # Return a zero vector if no words are found in the model\n",
        "\n",
        "    # Average the word vectors to get the sentence vector\n",
        "    sent_embedding = np.mean(word_vectors, axis=0)\n",
        "    return sent_embedding"
      ],
      "metadata": {
        "id": "yzU9pXzYa8-H"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach #2: compare sentence embeddings with each phrase and take max similarity"
      ],
      "metadata": {
        "id": "8A5LhOqFZ2xY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load the merged ambiguous words and sentences from the JSON file\n",
        "with open('ambigious_words_merged.json', 'r', encoding='utf-8') as f:\n",
        "    ambigious_words = json.load(f)\n",
        "\n",
        "# Word-sense disambiguation function using max similarity\n",
        "def calculate_max_sim(sent_embedding, data):\n",
        "    signal_phrases = data.get('signal_phrases', [])\n",
        "    if not signal_phrases:\n",
        "        return 0  # Return zero if there are no signal phrases\n",
        "\n",
        "    # Encode signal phrases to get their embeddings\n",
        "    phrase_embeddings = [get_sentence_embedding(phrase) for phrase in signal_phrases]\n",
        "\n",
        "    # Calculate cosine similarities between sentence embedding and all signal phrase embeddings\n",
        "    sims = cosine_similarity([sent_embedding], phrase_embeddings).flatten()\n",
        "\n",
        "    # Return the maximum similarity score\n",
        "    max_sim = sims.max()\n",
        "    return max_sim\n",
        "\n",
        "def disambiguator(word, sentence):\n",
        "    amb_word = ambigious_words.get(word)\n",
        "    if amb_word is None:\n",
        "        raise Exception(f\"Word '{word}' not found in our dictionary\")\n",
        "\n",
        "    # Encode the sentence to get its embedding\n",
        "    sent_embedding = get_sentence_embedding(sentence)\n",
        "\n",
        "    results = []\n",
        "    for meaning, data in amb_word.items():\n",
        "        max_score = calculate_max_sim(sent_embedding, data)\n",
        "        results.append((meaning, max_score))\n",
        "\n",
        "    # Sort the results by max similarity score in descending order\n",
        "    results = sorted(results, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Return the meaning with the highest max similarity score\n",
        "    return results[0][0]\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate():\n",
        "    count_correct = 0\n",
        "    count_all = 0\n",
        "    n = len(ambigious_words.keys())\n",
        "\n",
        "    # Iterate over each ambiguous word and its meanings\n",
        "    for amb_word, meanings in ambigious_words.items():\n",
        "        print(f\"{count_all} / {n}: amb_word = {amb_word}\")\n",
        "        for meaning, val in meanings.items():\n",
        "            for sent in val[\"sentences\"]:\n",
        "                answer = disambiguator(amb_word, sent)\n",
        "                count_all += 1\n",
        "                if answer == meaning:\n",
        "                    count_correct += 1\n",
        "\n",
        "    # Calculate and print accuracy\n",
        "    accuracy = count_correct / count_all\n",
        "    print(f\"Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "# Run the evaluation\n",
        "evaluate()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrmQeXmFZqz5",
        "outputId": "1780c798-3b24-4f7a-cecb-0a4f9329647a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 / 43: amb_word = ач\n",
            "2 / 43: amb_word = же\n",
            "4 / 43: amb_word = ак\n",
            "6 / 43: amb_word = кап\n",
            "9 / 43: amb_word = ала\n",
            "12 / 43: amb_word = кеч\n",
            "14 / 43: amb_word = кош\n",
            "16 / 43: amb_word = кал\n",
            "18 / 43: amb_word = бай\n",
            "20 / 43: amb_word = сай\n",
            "22 / 43: amb_word = арык\n",
            "24 / 43: amb_word = кой\n",
            "26 / 43: amb_word = ай\n",
            "29 / 43: amb_word = топ\n",
            "31 / 43: amb_word = жар\n",
            "34 / 43: amb_word = тил\n",
            "47 / 43: amb_word = каз\n",
            "49 / 43: amb_word = там\n",
            "51 / 43: amb_word = жаш\n",
            "54 / 43: amb_word = кара\n",
            "56 / 43: amb_word = мал\n",
            "58 / 43: amb_word = сөз\n",
            "60 / 43: amb_word = бас\n",
            "62 / 43: amb_word = тек\n",
            "64 / 43: amb_word = уч\n",
            "66 / 43: amb_word = жең\n",
            "68 / 43: amb_word = курак\n",
            "70 / 43: amb_word = айт\n",
            "72 / 43: amb_word = түш\n",
            "75 / 43: amb_word = кур\n",
            "78 / 43: amb_word = тай\n",
            "82 / 43: amb_word = кол\n",
            "84 / 43: amb_word = күн\n",
            "86 / 43: amb_word = ат\n",
            "89 / 43: amb_word = жаз\n",
            "93 / 43: amb_word = кат\n",
            "106 / 43: amb_word = сан\n",
            "108 / 43: amb_word = чал\n",
            "112 / 43: amb_word = кир\n",
            "115 / 43: amb_word = чек\n",
            "117 / 43: amb_word = бак\n",
            "119 / 43: amb_word = аябай\n",
            "121 / 43: amb_word = ачуу\n",
            "Accuracy = 0.4797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach #3: take embeddings of each words of the input sentence, calculate cosine similarity, take max value"
      ],
      "metadata": {
        "id": "fm-eaEAEbc2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load the merged ambiguous words and sentences from the JSON file\n",
        "with open('ambigious_words_merged.json', 'r', encoding='utf-8') as f:\n",
        "    ambigious_words = json.load(f)\n",
        "\n",
        "# Calculate max similarity by comparing each word's embedding in the sentence to the signal phrases\n",
        "def calculate_max_word_sim(sentence_words, data):\n",
        "    signal_phrases = data.get('signal_phrases', [])\n",
        "    if not signal_phrases:\n",
        "        return 0  # Return zero if there are no signal phrases\n",
        "\n",
        "    # Encode signal phrases to get their embeddings\n",
        "    phrase_embeddings = [get_sentence_embedding(phrase) for phrase in signal_phrases]\n",
        "\n",
        "    max_sim = 0  # Track the max similarity across all words in the sentence\n",
        "    for word in sentence_words:\n",
        "        # Encode each word in the sentence to get its embedding\n",
        "        word_embedding = get_sentence_embedding(word)\n",
        "\n",
        "        # Compute cosine similarity between the word embedding and signal phrase embeddings\n",
        "        sims = cosine_similarity([word_embedding], phrase_embeddings).flatten()\n",
        "\n",
        "        # Get the maximum similarity for this word and update the overall max if it's higher\n",
        "        word_max_sim = sims.max()\n",
        "        if word_max_sim > max_sim:\n",
        "            max_sim = word_max_sim\n",
        "\n",
        "    return max_sim\n",
        "\n",
        "# Word-sense disambiguation function with word-by-word comparison\n",
        "def disambiguator_word_level(word, sentence):\n",
        "    amb_word = ambigious_words.get(word)\n",
        "    if amb_word is None:\n",
        "        raise Exception(f\"Word '{word}' not found in our dictionary\")\n",
        "\n",
        "    # Split the sentence into words\n",
        "    sentence_words = sentence.split()\n",
        "\n",
        "    results = []\n",
        "    for meaning, data in amb_word.items():\n",
        "        max_score = calculate_max_word_sim(sentence_words, data)\n",
        "        results.append((meaning, max_score))\n",
        "\n",
        "    # Sort the results by max similarity score in descending order\n",
        "    results = sorted(results, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Return the meaning with the highest max similarity score\n",
        "    return results[0][0]\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_word_level():\n",
        "    count_correct = 0\n",
        "    count_all = 0\n",
        "    n = len(ambigious_words.keys())\n",
        "\n",
        "    # Iterate over each ambiguous word and its meanings\n",
        "    for amb_word, meanings in ambigious_words.items():\n",
        "        print(f\"{count_all} / {n}: amb_word = {amb_word}\")\n",
        "        for meaning, val in meanings.items():\n",
        "            for sent in val[\"sentences\"]:\n",
        "                answer = disambiguator_word_level(amb_word, sent)\n",
        "                count_all += 1\n",
        "                if answer == meaning:\n",
        "                    count_correct += 1\n",
        "\n",
        "    # Calculate and print accuracy\n",
        "    accuracy = count_correct / count_all\n",
        "    print(f\"Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "# Run the evaluation using the word-level comparison approach\n",
        "evaluate_word_level()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCMp0YpmaS0r",
        "outputId": "72496683-e5c9-4a1b-c9cf-15bd57716af0"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 / 43: amb_word = ач\n",
            "2 / 43: amb_word = же\n",
            "4 / 43: amb_word = ак\n",
            "6 / 43: amb_word = кап\n",
            "9 / 43: amb_word = ала\n",
            "12 / 43: amb_word = кеч\n",
            "14 / 43: amb_word = кош\n",
            "16 / 43: amb_word = кал\n",
            "18 / 43: amb_word = бай\n",
            "20 / 43: amb_word = сай\n",
            "22 / 43: amb_word = арык\n",
            "24 / 43: amb_word = кой\n",
            "26 / 43: amb_word = ай\n",
            "29 / 43: amb_word = топ\n",
            "31 / 43: amb_word = жар\n",
            "34 / 43: amb_word = тил\n",
            "47 / 43: amb_word = каз\n",
            "49 / 43: amb_word = там\n",
            "51 / 43: amb_word = жаш\n",
            "54 / 43: amb_word = кара\n",
            "56 / 43: amb_word = мал\n",
            "58 / 43: amb_word = сөз\n",
            "60 / 43: amb_word = бас\n",
            "62 / 43: amb_word = тек\n",
            "64 / 43: amb_word = уч\n",
            "66 / 43: amb_word = жең\n",
            "68 / 43: amb_word = курак\n",
            "70 / 43: amb_word = айт\n",
            "72 / 43: amb_word = түш\n",
            "75 / 43: amb_word = кур\n",
            "78 / 43: amb_word = тай\n",
            "82 / 43: amb_word = кол\n",
            "84 / 43: amb_word = күн\n",
            "86 / 43: amb_word = ат\n",
            "89 / 43: amb_word = жаз\n",
            "93 / 43: amb_word = кат\n",
            "106 / 43: amb_word = сан\n",
            "108 / 43: amb_word = чал\n",
            "112 / 43: amb_word = кир\n",
            "115 / 43: amb_word = чек\n",
            "117 / 43: amb_word = бак\n",
            "119 / 43: amb_word = аябай\n",
            "121 / 43: amb_word = ачуу\n",
            "Accuracy = 0.4553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach #4: embedding of a sliding window of size 2"
      ],
      "metadata": {
        "id": "6vfAOi_LcHG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the merged ambiguous words and sentences from the JSON file\n",
        "with open('ambigious_words_merged.json', 'r', encoding='utf-8') as f:\n",
        "    ambigious_words = json.load(f)\n",
        "\n",
        "# Helper function to get the average embedding for a 2-word window\n",
        "def get_window_embedding(words):\n",
        "    word_vectors = [model.get_word_vector(word) for word in words if word in model]\n",
        "\n",
        "    if len(word_vectors) == 0:\n",
        "        return np.zeros(300)  # Return a zero vector if no words are found in the model\n",
        "\n",
        "    # Average the word vectors to get the window embedding\n",
        "    window_embedding = np.mean(word_vectors, axis=0)\n",
        "    return window_embedding\n",
        "\n",
        "# Calculate max similarity by comparing each 2-word window's embedding in the sentence to the signal phrases\n",
        "def calculate_max_window_sim(sentence_words, data):\n",
        "    signal_phrases = data.get('signal_phrases', [])\n",
        "    if not signal_phrases:\n",
        "        return 0  # Return zero if there are no signal phrases\n",
        "\n",
        "    # Get embeddings for each signal phrase\n",
        "    phrase_embeddings = [get_window_embedding(phrase.split()) for phrase in signal_phrases]\n",
        "\n",
        "    max_sim = 0  # Track the max similarity across all 2-word windows in the sentence\n",
        "    # Sliding window of 2 words\n",
        "    for i in range(len(sentence_words) - 1):\n",
        "        # Create a 2-word window\n",
        "        window_words = sentence_words[i:i+2]\n",
        "\n",
        "        # Get the embedding for the 2-word window\n",
        "        window_embedding = get_window_embedding(window_words)\n",
        "\n",
        "        # Compute cosine similarity between the 2-word window embedding and signal phrase embeddings\n",
        "        sims = cosine_similarity([window_embedding], phrase_embeddings).flatten()\n",
        "\n",
        "        # Get the maximum similarity for this window and update the overall max if it's higher\n",
        "        window_max_sim = sims.max()\n",
        "        if window_max_sim > max_sim:\n",
        "            max_sim = window_max_sim\n",
        "\n",
        "    return max_sim\n",
        "\n",
        "# Word-sense disambiguation function with sliding window of 2 words\n",
        "def disambiguator_window_level(word, sentence):\n",
        "    amb_word = ambigious_words.get(word)\n",
        "    if amb_word is None:\n",
        "        raise Exception(f\"Word '{word}' not found in our dictionary\")\n",
        "\n",
        "    # Split the sentence into words\n",
        "    sentence_words = sentence.split()\n",
        "\n",
        "    results = []\n",
        "    for meaning, data in amb_word.items():\n",
        "        max_score = calculate_max_window_sim(sentence_words, data)\n",
        "        results.append((meaning, max_score))\n",
        "\n",
        "    # Sort the results by max similarity score in descending order\n",
        "    results = sorted(results, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Return the meaning with the highest max similarity score\n",
        "    return results[0][0]\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_window_level():\n",
        "    count_correct = 0\n",
        "    count_all = 0\n",
        "    n = len(ambigious_words.keys())\n",
        "\n",
        "    # Iterate over each ambiguous word and its meanings\n",
        "    for amb_word, meanings in ambigious_words.items():\n",
        "        print(f\"{count_all} / {n}: amb_word = {amb_word}\")\n",
        "        for meaning, val in meanings.items():\n",
        "            for sent in val[\"sentences\"]:\n",
        "                answer = disambiguator_window_level(amb_word, sent)\n",
        "                count_all += 1\n",
        "                if answer == meaning:\n",
        "                    count_correct += 1\n",
        "\n",
        "    # Calculate and print accuracy\n",
        "    accuracy = count_correct / count_all\n",
        "    print(f\"Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "# Run the evaluation using the sliding window of 2-word approach\n",
        "evaluate_window_level()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gk0ErvYLb46s",
        "outputId": "39fcc8d2-d5c5-44a3-b987-029903574b4a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 / 43: amb_word = ач\n",
            "2 / 43: amb_word = же\n",
            "4 / 43: amb_word = ак\n",
            "6 / 43: amb_word = кап\n",
            "9 / 43: amb_word = ала\n",
            "12 / 43: amb_word = кеч\n",
            "14 / 43: amb_word = кош\n",
            "16 / 43: amb_word = кал\n",
            "18 / 43: amb_word = бай\n",
            "20 / 43: amb_word = сай\n",
            "22 / 43: amb_word = арык\n",
            "24 / 43: amb_word = кой\n",
            "26 / 43: amb_word = ай\n",
            "29 / 43: amb_word = топ\n",
            "31 / 43: amb_word = жар\n",
            "34 / 43: amb_word = тил\n",
            "47 / 43: amb_word = каз\n",
            "49 / 43: amb_word = там\n",
            "51 / 43: amb_word = жаш\n",
            "54 / 43: amb_word = кара\n",
            "56 / 43: amb_word = мал\n",
            "58 / 43: amb_word = сөз\n",
            "60 / 43: amb_word = бас\n",
            "62 / 43: amb_word = тек\n",
            "64 / 43: amb_word = уч\n",
            "66 / 43: amb_word = жең\n",
            "68 / 43: amb_word = курак\n",
            "70 / 43: amb_word = айт\n",
            "72 / 43: amb_word = түш\n",
            "75 / 43: amb_word = кур\n",
            "78 / 43: amb_word = тай\n",
            "82 / 43: amb_word = кол\n",
            "84 / 43: amb_word = күн\n",
            "86 / 43: amb_word = ат\n",
            "89 / 43: amb_word = жаз\n",
            "93 / 43: amb_word = кат\n",
            "106 / 43: amb_word = сан\n",
            "108 / 43: amb_word = чал\n",
            "112 / 43: amb_word = кир\n",
            "115 / 43: amb_word = чек\n",
            "117 / 43: amb_word = бак\n",
            "119 / 43: amb_word = аябай\n",
            "121 / 43: amb_word = ачуу\n",
            "Accuracy = 0.4797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach #5: sliding window embeddings of size 3"
      ],
      "metadata": {
        "id": "VrlRJrcteH5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the merged ambiguous words and sentences from the JSON file\n",
        "with open('ambigious_words_merged.json', 'r', encoding='utf-8') as f:\n",
        "    ambigious_words = json.load(f)\n",
        "\n",
        "# Helper function to get the average embedding for a 3-word window\n",
        "def get_window_embedding(words):\n",
        "    word_vectors = [model.get_word_vector(word) for word in words if word in model]\n",
        "\n",
        "    if len(word_vectors) == 0:\n",
        "        return np.zeros(300)  # Return a zero vector if no words are found in the model\n",
        "\n",
        "    # Average the word vectors to get the window embedding\n",
        "    window_embedding = np.mean(word_vectors, axis=0)\n",
        "    return window_embedding\n",
        "\n",
        "# Calculate max similarity by comparing each 3-word window's embedding in the sentence to the signal phrases\n",
        "def calculate_max_window_sim(sentence_words, data):\n",
        "    signal_phrases = data.get('signal_phrases', [])\n",
        "    if not signal_phrases:\n",
        "        return 0  # Return zero if there are no signal phrases\n",
        "\n",
        "    # Get embeddings for each signal phrase\n",
        "    phrase_embeddings = [get_window_embedding(phrase.split()) for phrase in signal_phrases]\n",
        "\n",
        "    max_sim = 0  # Track the max similarity across all 3-word windows in the sentence\n",
        "    # Sliding window of 3 words\n",
        "    for i in range(len(sentence_words) - 2):  # Adjusted to ensure a 3-word window\n",
        "        # Create a 3-word window\n",
        "        window_words = sentence_words[i:i+3]\n",
        "\n",
        "        # Get the embedding for the 3-word window\n",
        "        window_embedding = get_window_embedding(window_words)\n",
        "\n",
        "        # Compute cosine similarity between the 3-word window embedding and signal phrase embeddings\n",
        "        sims = cosine_similarity([window_embedding], phrase_embeddings).flatten()\n",
        "\n",
        "        # Get the maximum similarity for this window and update the overall max if it's higher\n",
        "        window_max_sim = sims.max()\n",
        "        if window_max_sim > max_sim:\n",
        "            max_sim = window_max_sim\n",
        "\n",
        "    return max_sim\n",
        "\n",
        "# Word-sense disambiguation function with sliding window of 3 words\n",
        "def disambiguator_window_level(word, sentence):\n",
        "    amb_word = ambigious_words.get(word)\n",
        "    if amb_word is None:\n",
        "        raise Exception(f\"Word '{word}' not found in our dictionary\")\n",
        "\n",
        "    # Split the sentence into words\n",
        "    sentence_words = sentence.split()\n",
        "\n",
        "    results = []\n",
        "    for meaning, data in amb_word.items():\n",
        "        max_score = calculate_max_window_sim(sentence_words, data)\n",
        "        results.append((meaning, max_score))\n",
        "\n",
        "    # Sort the results by max similarity score in descending order\n",
        "    results = sorted(results, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Return the meaning with the highest max similarity score\n",
        "    return results[0][0]\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_window_level():\n",
        "    count_correct = 0\n",
        "    count_all = 0\n",
        "    n = len(ambigious_words.keys())\n",
        "\n",
        "    # Iterate over each ambiguous word and its meanings\n",
        "    for amb_word, meanings in ambigious_words.items():\n",
        "        print(f\"{count_all} / {n}: amb_word = {amb_word}\")\n",
        "        for meaning, val in meanings.items():\n",
        "            for sent in val[\"sentences\"]:\n",
        "                answer = disambiguator_window_level(amb_word, sent)\n",
        "                count_all += 1\n",
        "                if answer == meaning:\n",
        "                    count_correct += 1\n",
        "\n",
        "    # Calculate and print accuracy\n",
        "    accuracy = count_correct / count_all\n",
        "    print(f\"Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "# Run the evaluation using the sliding window of 3-word approach\n",
        "evaluate_window_level()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FYh6GmPcfCM",
        "outputId": "291986a3-157b-4f1a-d9b2-772cc8bba65f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 / 43: amb_word = ач\n",
            "2 / 43: amb_word = же\n",
            "4 / 43: amb_word = ак\n",
            "6 / 43: amb_word = кап\n",
            "9 / 43: amb_word = ала\n",
            "12 / 43: amb_word = кеч\n",
            "14 / 43: amb_word = кош\n",
            "16 / 43: amb_word = кал\n",
            "18 / 43: amb_word = бай\n",
            "20 / 43: amb_word = сай\n",
            "22 / 43: amb_word = арык\n",
            "24 / 43: amb_word = кой\n",
            "26 / 43: amb_word = ай\n",
            "29 / 43: amb_word = топ\n",
            "31 / 43: amb_word = жар\n",
            "34 / 43: amb_word = тил\n",
            "47 / 43: amb_word = каз\n",
            "49 / 43: amb_word = там\n",
            "51 / 43: amb_word = жаш\n",
            "54 / 43: amb_word = кара\n",
            "56 / 43: amb_word = мал\n",
            "58 / 43: amb_word = сөз\n",
            "60 / 43: amb_word = бас\n",
            "62 / 43: amb_word = тек\n",
            "64 / 43: amb_word = уч\n",
            "66 / 43: amb_word = жең\n",
            "68 / 43: amb_word = курак\n",
            "70 / 43: amb_word = айт\n",
            "72 / 43: amb_word = түш\n",
            "75 / 43: amb_word = кур\n",
            "78 / 43: amb_word = тай\n",
            "82 / 43: amb_word = кол\n",
            "84 / 43: amb_word = күн\n",
            "86 / 43: amb_word = ат\n",
            "89 / 43: amb_word = жаз\n",
            "93 / 43: amb_word = кат\n",
            "106 / 43: amb_word = сан\n",
            "108 / 43: amb_word = чал\n",
            "112 / 43: amb_word = кир\n",
            "115 / 43: amb_word = чек\n",
            "117 / 43: amb_word = бак\n",
            "119 / 43: amb_word = аябай\n",
            "121 / 43: amb_word = ачуу\n",
            "Accuracy = 0.4634\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8OCLiBdoeaWt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}