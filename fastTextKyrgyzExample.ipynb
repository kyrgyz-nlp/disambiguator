{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-3Pvjw3ZlPnz",
    "outputId": "778d83b9-5ae5-4552-c2f5-f27519dd8396"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fasttext\n",
      "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting pybind11>=2.2 (from fasttext)\n",
      "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (71.0.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.26.4)\n",
      "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fasttext: filename=fasttext-0.9.3-cp310-cp310-linux_x86_64.whl size=4296184 sha256=789b3feec32d3b1930478b82c8690b0dfc00faf01a8c5d804fb861c1e758feed\n",
      "  Stored in directory: /root/.cache/pip/wheels/0d/a2/00/81db54d3e6a8199b829d58e02cec2ddb20ce3e59fad8d3c92a\n",
      "Successfully built fasttext\n",
      "Installing collected packages: pybind11, fasttext\n",
      "Successfully installed fasttext-0.9.3 pybind11-2.13.6\n"
     ]
    }
   ],
   "source": [
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "53hj_OUwmaea",
    "outputId": "240560ee-66fd-4de3-d785-2f0cd4cb87f5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext.util\n",
    "# Жарым саат ~ бир саат жүктөйт\n",
    "fasttext.util.download_model('ky', if_exists='ignore')  # Kyrgyz\n",
    "model = fasttext.load_model('cc.ky.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qKvAJUvwlPKX"
   },
   "outputs": [],
   "source": [
    "horse = model.get_sentence_vector('Ат күчтүү жана ылдам.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bopvKVeVtvDQ"
   },
   "outputs": [],
   "source": [
    "name = model.get_sentence_vector(\"Биздин топко жаңы адамдын атын кош.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OyZqLHpzuONk",
    "outputId": "7d5bdd7d-865a-436f-e75e-76e4820f6e62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.37710795]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity = cosine_similarity([horse], [name])\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93v0q67Iuf7c",
    "outputId": "f29e126a-9c9c-4913-f5b0-32dcdf5f964c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.47417092]]\n"
     ]
    }
   ],
   "source": [
    "horse2 = model.get_sentence_vector(\"Ат тосмодон секирип өттү.\")\n",
    "\n",
    "similarity = cosine_similarity([horse], [horse2])\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cF3ZNg-Nu9J3",
    "outputId": "1eff7011-74e4-4a05-e84f-745fde34d9af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5634349]]\n"
     ]
    }
   ],
   "source": [
    "name2 = model.get_sentence_vector(\"Клубдун атын ким ойлоп тапты?\")\n",
    "\n",
    "similarity = cosine_similarity([name], [name2])\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qjp0fht4vRtC",
    "outputId": "38ce2e75-aa23-4592-cfd1-8a68e69e0f32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2855695]]\n"
     ]
    }
   ],
   "source": [
    "similarity = cosine_similarity([horse2], [name2])\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tL-uxVjrvrTD"
   },
   "outputs": [],
   "source": [
    "horse_sentences = [\n",
    "    \"Ат талаада чуркап баратат.\", \"Кечээ мен ат миндим.\", \"Атты сатып алгам.\",\n",
    "    \"Дыйкан атка жем берүүдө.\", \"Ат тосмодон секирип өттү.\", \"Аттар сулуу жаныбарлар.\",\n",
    "    \"Аттын жалынын кылдары бекем болот.\", \"Ат чөп жейт, бирок жемди көбүрөөк жакшы көрөт.\",\n",
    "    \"Балдар ат минип жүргөндү жакшы көрүшөт.\", \"Ат катуу кишенеди.\", \"Ал жаңы ат сатып алды.\",\n",
    "    \"Ат мөңкүп жатып, чабарманды ыргытып жиберди.\", \"Биз жапайы ат көрдүк.\", \"Кара ат жарышта жеңди.\",\n",
    "    \"Ат сарайда оонап жатат!\", \"Ал аттын жүнүн тарап койду.\", \"Ат арабаны тартып баратат.\", \"Алардын үч аты бар.\",\n",
    "    \"Ат күчтүү жана ылдам.\", \"Ат жана чабандес бирдей кыймылдады.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "YP0yFEqLxgAL"
   },
   "outputs": [],
   "source": [
    "test_sentences = [\"Чаптырып жүргөн атын уурдатып алыптыр\", \"Аты ала качып кетип, жыгылган экен\", \"Ашка ат союлду.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "m4NL4q7zy1qT"
   },
   "outputs": [],
   "source": [
    "embeddings = [model.get_sentence_vector(sentence) for sentence in horse_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4bqKr-tMz81h",
    "outputId": "3a7d1101-b018-45d4-fcc1-95c32eb38fd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Similarity Score: 0.38370108668643393\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "pairs = list(itertools.combinations(embeddings, 2))\n",
    "similarities = [cosine_similarity([pair[0]], [pair[1]])[0][0] for pair in pairs]\n",
    "# Calculate average similarity\n",
    "average_similarity = sum(similarities) / len(similarities)\n",
    "print(f\"Average Similarity Score: {average_similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Yyoz7HJ30LmF"
   },
   "outputs": [],
   "source": [
    "shoot_sentences = [\"Сарбаз бутага так атты.\", \"Жаачы жебе атканга даярдады.\", \"Мергенчи бугуну атпай койду.\",\n",
    "\"Ал пистолетти колго алып, ат деп буйрук берди.\", \"Жоокерлер душманды көргөндө ата башташты.\",\n",
    "\"Күзөтчү абага атты.\", \"Мергенчи мылтык менен кушту атты.\", \"Ал бутага көздөй атты.\", \"Бала ойунчук куралдан суу атты.\",\n",
    "\"Башаламандыктарда бир нече жолу ок атылган.\", \"Жакын келбегиле, атам!\", \"Душмандарды коркутуу үчүн асманга ок атылды.\",\n",
    "\"Тынч эмес аймакта бир нече жолу атышуу болду.\", \"Күч органдары кылмышкерди атып салышкан.\", \"Ал ызы-чуу чыкканда абага ок атты.\",\n",
    "\"Полиция шектүүнү аткан жок.\", \"Совет бийлигинин буйругу менен атууга кеткет.\", \"Аскерлер буйрук күтүп, атпай турду.\", \"Кечке бутага аттык.\",\n",
    "\"Лагерге айдалып, андан ары атылып кеткен.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "5a3xWelg1Bqj"
   },
   "outputs": [],
   "source": [
    "embeddings = [model.get_sentence_vector(sentence) for sentence in shoot_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1aN7VLrK1FCx",
    "outputId": "f004eb45-4677-4344-f48b-2287d712d38e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Similarity Score: 0.4510482430458069\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "pairs = list(itertools.combinations(embeddings, 2))\n",
    "similarities = [cosine_similarity([pair[0]], [pair[1]])[0][0] for pair in pairs]\n",
    "# Calculate average similarity\n",
    "average_similarity = sum(similarities) / len(similarities)\n",
    "print(f\"Average Similarity Score: {average_similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BoSLSRvs1GiD",
    "outputId": "b4fe2ccf-9426-48b3-b17c-d1727852d5fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.46285263]]\n"
     ]
    }
   ],
   "source": [
    "shoot_sent = model.get_sentence_vector(shoot_sentences[5])\n",
    "horse_sent = model.get_sentence_vector(horse_sentences[7])\n",
    "sim = cosine_similarity([shoot_sent], [horse_sent])\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "aOo7iP8T2qVl",
    "outputId": "a0e05565-9288-4faf-dab3-344d58fd64c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ал ызы-чуу чыкканда абага ок атты.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shoot_sentences[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "VDax9i4T6XC3",
    "outputId": "a7030626-aecd-4f12-882c-1c3e7152c581"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Атты сатып алгам.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "horse_sentences[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uN5iXVm-1O-X",
    "outputId": "69409360-316d-4673-bbbb-5c8e61b30b33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.31485605]]\n"
     ]
    }
   ],
   "source": [
    "shoot_sent = model.get_sentence_vector(shoot_sentences[14])\n",
    "horse_sent = model.get_sentence_vector(horse_sentences[2])\n",
    "sim = cosine_similarity([shoot_sent], [horse_sent])\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hh5j1ncr7G7r"
   },
   "source": [
    "## We want to use a signal phrase that should check if it's closer to \"horses\" or \"shooting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "TT416I-L6tXW"
   },
   "outputs": [],
   "source": [
    "signal_phrase_horse = \"жем берди\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Go3gJMVC61qN",
    "outputId": "9da355ec-d11a-42c2-d24c-0e233c3d3dad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25874266]]\n"
     ]
    }
   ],
   "source": [
    "shoot_sent = model.get_sentence_vector(shoot_sentences[14])\n",
    "signal_phrase_horse_vec = model.get_sentence_vector(signal_phrase_horse)\n",
    "sim = cosine_similarity([shoot_sent], [signal_phrase_horse_vec])\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X1qdrHFV44sr",
    "outputId": "679a899b-54f6-45ec-ff00-72c33f00a85d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.41549802]]\n"
     ]
    }
   ],
   "source": [
    "horse_sent = model.get_sentence_vector(horse_sentences[2])\n",
    "sim = cosine_similarity([horse_sent], [signal_phrase_horse_vec])\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdu5w1mQ8Swo"
   },
   "source": [
    "## Use multiple signal phrases\n",
    "\n",
    "1. We have a dictionary of homonyms. Against every meaning there are 1) example sentences 2) multiple signal phrases: [\"чаптыруу\", \"ээр токун\", \"улак тартыш\"]\n",
    "2. Every time we get a sentence containing an ambigious word, we get its embeddings and compare against signal phrases of every meaning and see which one wins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "5vt_Kb0t7gb9"
   },
   "outputs": [],
   "source": [
    "ambigious_words = {\n",
    "    \"ат\": {\n",
    "        \"name\": {\n",
    "            \"signal_phrases\": [\"ысым бер\", \"фамилия\", \"адам\"],\n",
    "        },\n",
    "        \"horse\": {\n",
    "            \"signal_phrases\": [\"чаптыруу\", \"ээр токун\", \"улак тартыш\"],\n",
    "        },\n",
    "        \"shoot\": {\n",
    "            \"signal_phrases\": [\"ок атуу\", \"мылтык\", \"автомат\", \"пистолет\", \"жаа\", \"аскер\", \"полиция\"],\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ahDNMOBQFDsu"
   },
   "outputs": [],
   "source": [
    "def calculate_avg_sim(sent_embedding, data):\n",
    "    signal_phrases = data.get('signal_phrases', [])\n",
    "    if not signal_phrases:\n",
    "        return 0  # Return zero if there are no signal phrases\n",
    "    sims = []\n",
    "    for phrase in signal_phrases:\n",
    "        phrase_embedding = model.get_sentence_vector(phrase)\n",
    "        sim = cosine_similarity([sent_embedding], [phrase_embedding])[0][0]\n",
    "        sims.append(sim)\n",
    "    avg_sim = sum(sims) / len(sims)\n",
    "    return avg_sim\n",
    "\n",
    "def disambiguator(word, sentence):\n",
    "    amb_word = ambigious_words.get(word)\n",
    "    if amb_word is None:\n",
    "        raise Exception(f\"Word '{word}' not found in our dictionary\")\n",
    "    results = []\n",
    "    sent_embedding = model.get_sentence_vector(sentence)\n",
    "    for meaning, data in amb_word.items():\n",
    "        avg_score = calculate_avg_sim(sent_embedding, data)\n",
    "        results.append((meaning, avg_score))\n",
    "    # Sort the results in descending order of average similarity\n",
    "    results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "    # Return the meaning with the highest average similarity score\n",
    "    return results[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TlTZE7FBFFCY",
    "outputId": "be18d315-2bdc-4063-e36f-2631ece915bc"
   },
   "outputs": [],
   "source": [
    "# dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XLb_4frDFcAO",
    "outputId": "55b8a5bf-261a-42bd-d2c3-becd2ec6d6a8"
   },
   "outputs": [],
   "source": [
    "# model.get_nearest_neighbors(\"атты\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_exeoxpFg8N"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_file = \"sentences.json\"\n",
    "with open(json_file, 'r', encoding='utf-8') as f:\n",
    "    sentences = json.load(f)\n",
    "\n",
    "    \n",
    "json_file_ambiguous_words_50 = \"ambiguous_words_50.json\"\n",
    "with open(json_file_ambiguous_words_50, 'r', encoding='utf-8') as f:\n",
    "    ambiguous_words_50 = json.load(f)\n",
    "\n",
    "\n",
    "set(sentences.keys()) == set(ambiguous_words_50.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "containers of amb_word 'там' in json mismatch! This key is deleted.\n"
     ]
    }
   ],
   "source": [
    "# Merge JSONs\n",
    "ambigious_words = ambiguous_words_50.copy()\n",
    "for amb_word, meanings in ambiguous_words_50.items():\n",
    "    if set(meanings.keys()) != set(sentences[amb_word].keys()):\n",
    "        print(f\"containers of amb_word '{amb_word}' in json mismatch! This key is deleted.\")\n",
    "        del ambigious_words[amb_word]\n",
    "        continue\n",
    "    for meaning in meanings.keys():\n",
    "        ambigious_words[amb_word][meaning][\"sentences\"] = sentences[amb_word][meaning][\"sentences\"]\n",
    "\n",
    "# Save to json\n",
    "with open('ambigious_words.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(ambigious_words, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 42: amb_word = ач\n",
      "2 / 42: amb_word = же\n",
      "4 / 42: amb_word = ак\n",
      "6 / 42: amb_word = кап\n",
      "9 / 42: amb_word = ала\n",
      "12 / 42: amb_word = кеч\n",
      "14 / 42: amb_word = кош\n",
      "16 / 42: amb_word = кал\n",
      "18 / 42: amb_word = бай\n",
      "20 / 42: amb_word = сай\n",
      "22 / 42: amb_word = арык\n",
      "24 / 42: amb_word = кой\n",
      "26 / 42: amb_word = ай\n",
      "29 / 42: amb_word = топ\n",
      "31 / 42: amb_word = жар\n",
      "34 / 42: amb_word = тил\n",
      "47 / 42: amb_word = каз\n",
      "49 / 42: amb_word = жаш\n",
      "52 / 42: amb_word = кара\n",
      "54 / 42: amb_word = мал\n",
      "56 / 42: amb_word = сөз\n",
      "58 / 42: amb_word = бас\n",
      "60 / 42: amb_word = тек\n",
      "62 / 42: amb_word = уч\n",
      "64 / 42: amb_word = жең\n",
      "66 / 42: amb_word = курак\n",
      "68 / 42: amb_word = айт\n",
      "70 / 42: amb_word = түш\n",
      "73 / 42: amb_word = кур\n",
      "76 / 42: amb_word = тай\n",
      "84 / 42: amb_word = кол\n",
      "86 / 42: amb_word = күн\n",
      "88 / 42: amb_word = ат\n",
      "91 / 42: amb_word = жаз\n",
      "95 / 42: amb_word = кат\n",
      "108 / 42: amb_word = сан\n",
      "110 / 42: amb_word = чал\n",
      "114 / 42: amb_word = кир\n",
      "117 / 42: amb_word = чек\n",
      "119 / 42: amb_word = бак\n",
      "121 / 42: amb_word = аябай\n",
      "123 / 42: amb_word = ачуу\n",
      "CPU times: total: 2.14 s\n",
      "Wall time: 2.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "count_correct = 0\n",
    "count_all = 0\n",
    "n = len(ambigious_words.keys())\n",
    "for amb_word, meanings in ambigious_words.items():\n",
    "    print(f\"{count_all} / {n}: amb_word = {amb_word}\")\n",
    "    for meaning, val in meanings.items():\n",
    "        for sent in val[\"sentences\"]:\n",
    "            answer = disambiguator(amb_word, sent)\n",
    "            count_all += 1\n",
    "            if answer == meaning:\n",
    "                count_correct += 1\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.544\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy = {count_correct / count_all}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
